# -*- coding: utf-8 -*-
"""pre-eclampsia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GzUNqhx2FiFnK2uwWE_J9MjkFgTxfez3
"""

import pandas as pd
import numpy as np

df = pd.read_csv("Pre-eclampsia - Sheet1.csv")
df.head()

# Data Cleaning and Preprocessing

# Replacing '-' with NaN for better handling of missing data
df.replace("-",np.NaN, inplace = True)

# Extract systolic and diastolic blood pressure values
df[['systolic_bp', 'diastolic_bp']] = df['Bp(mmhg)'].str.split('/', expand=True)
df.drop(columns=['Bp(mmhg)','diastolic'], inplace=True)

# Convert numerical columns to appropriate data types
df['systolic_bp'] = pd.to_numeric(df['systolic_bp'], errors='coerce')
df['diastolic_bp'] = pd.to_numeric(df['diastolic_bp'], errors='coerce')

# chnage the pcs column to string
df['Pcv'] = df['Pcv'].astype(str)
df['Pcv'] = df['Pcv'].str.rstrip("%").astype('float') / 100.0  # Convert to fraction
df['Gestational Age'] = df['Gestational Age'].astype(str)
df['Gestational Age'] = df['Gestational Age'].str.rstrip('weeks').astype('float')

# Convert categorical columns to appropriate data types
categorical_columns = ['Urinalysis', 'Blood Group(BG)', 'HCV', 'RVS', 'HBV', 'HBSAG', 'Pre-Eclampsia(yes/no)']
df[categorical_columns] = df[categorical_columns].astype('category')

# Display the cleaned dataset
df.head()



"""df download"""
#
# from google.colab import files
# df.to_csv('my_dataframe.csv', index=False)  # Set index=False if you don't want to include row indices
#
# files.download('my_dataframe.csv')

"""Lets analyse the column"""

# Data Cleaning Process

# Checking for duplicate columns and any irrelevant columns
print("Column names:", df.columns)

# Checking for the number of missing values in each column
missing_values = df.isnull().sum()

# Checking data types of each column
data_types = df.dtypes

missing_values, data_types

# Simplifying column names
df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')

# Dropping the 'systolic' column if it's a duplicate of 'systolic_bp'
if 'systolic' in df.columns and 'systolic_bp' in df.columns:
    if df['systolic'].equals(df['systolic_bp']):
        df.drop('systolic', axis=1, inplace=True)
    else:
        # If they are not duplicates, we need to decide what to do with these columns
        print("systolic and systolic_bp are not duplicates. Further inspection needed.")

# Dealing with missing values
# We'll first check the percentage of missing values in each column to decide on the approach
missing_percentage = df.isnull().sum() / len(df) * 100

missing_percentage

"""The dataset has been cleaned to some extent, with missing values in 'Pcv', 'Gestational_Age', 'systolic_bp', and 'diastolic_bp' addressed through imputation. However, there are still columns with a significant amount of missing data:

Urinalysis
Blood_GroupBG
HCV
RVS
PRbm
History_of_emclapsia_in_family
HBSAG
Given the high percentage of missing values in these columns, we have a few options:

If these variables are crucial for predicting pre-eclampsia, we might consider imputation, though it might introduce bias due to the high volume of missing data.
If they are not critical, we might consider dropping these columns.
For a predictive model, it's also essential to encode categorical variables. Variables like 'Urinalysis', 'Blood_GroupBG', 'HCV', 'RVS', 'History_of_emclapsia_in_family', 'HBSAG', and 'Pre-Eclampsiayes/no' are categorical and will require encoding.

Let's proceed with encoding these categorical variables. After that, I will assess the dataset and suggest the best approach for handling the remaining missing values and moving forward with the analysis.
"""

# Dropping columns with a high percentage of missing values or irrelevance
columns_to_drop = ['HBV', 'systolic']
df.drop(columns=columns_to_drop, inplace=True)

# Deciding on a strategy for other columns with missing data
# For columns with a moderate amount of missing data, we'll use imputation
# For columns with a high percentage of missing data, we'll evaluate their importance

# Imputing missing values for 'Pcv' and 'Gestational_Age' with their median values
df['Pcv'].fillna(df['Pcv'].median(), inplace=True)
df['Gestational_Age'].fillna(df['Gestational_Age'].median(), inplace=True)

# Imputing missing values for blood pressure with median values
df['systolic_bp'].fillna(df['systolic_bp'].median(), inplace=True)
df['diastolic_bp'].fillna(df['diastolic_bp'].median(), inplace=True)

# Checking the updated dataset
updated_missing_percentage = df.isnull().sum() / len(df) * 100
updated_missing_percentage, df.head()

from sklearn.preprocessing import LabelEncoder

# Creating a label encoder object
le = LabelEncoder()

# Encoding categorical variables
categorical_columns = ['Urinalysis', 'Blood_GroupBG', 'HCV', 'RVS', 'History_of_emclapsia_in_family', 'HBSAG', 'Pre-Eclampsiayes/no']
for col in categorical_columns:
    if col in df.columns:
        # Convert to 'category' dtype and include 'Missing' in categories if not already included
        if df[col].dtype != 'category':
            df[col] = df[col].astype('category')
        if 'Missing' not in df[col].cat.categories:
            df[col] = df[col].cat.add_categories('Missing')

        # Fill missing values with 'Missing'
        df[col].fillna('Missing', inplace=True)

        # Encode the categorical data
        df[col] = le.fit_transform(df[col])

# Checking the dataset after encoding
encoded_data = df
encoded_data.head()

"""Let's proceed with an exploratory data analysis (EDA) to understand the relationships and patterns within the data, particularly focusing on how different variables might relate to the prediction of pre-eclampsia. We'll look at distributions of individual variables and explore correlations and potential relationships with the target variable 'Pre-Eclampsiayes/no'.

We'll start with basic descriptive statistics, then examine distributions and correlations, and finally visualize some key relationships.
"""



import matplotlib.pyplot as plt
import seaborn as sns

# Basic Descriptive Statistics
descriptive_stats = encoded_data.describe()

# Correlation Matrix
correlation_matrix = encoded_data.corr()

# Plotting the Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)
plt.title("Correlation Matrix")
plt.show()

descriptive_stats, correlation_matrix['Pre-Eclampsiayes/no'].sort_values(ascending=False)

"""Lets visualise the distribution of PRbm to decide wether to use mean, or median for imputation."""

# Visualizing the distribution of 'PRbm'
plt.figure(figsize=(8, 6))
sns.histplot(encoded_data['PRbm'].dropna(), kde=True)
plt.title("Distribution of PRbm")
plt.xlabel("PRbm")
plt.ylabel("Frequency")
plt.show()

# Checking skewness
prbm_skewness = encoded_data['PRbm'].skew()
prbm_skewness

"""Impute the missing values of PRbm using a statistical method and include it in the predictive model."""

# Imputing missing values in 'PRbm' with its median value
prbm_median = encoded_data['PRbm'].median()
encoded_data['PRbm'].fillna(prbm_median, inplace=True)

# Checking if there are any more missing values in the dataset
remaining_missing_values = encoded_data.isnull().sum().sum()
remaining_missing_values



"""lest split the data into training and testing"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Defining the features and target variable
X = encoded_data.drop('Pre-Eclampsiayes/no', axis=1)
y = encoded_data['Pre-Eclampsiayes/no']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape

"""lets build a model using Logistic regression and Random Forest"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix

# Building the Logistic Regression model
log_reg = LogisticRegression()
log_reg.fit(X_train_scaled, y_train)

# Making predictions on the test set
y_pred_log_reg = log_reg.predict(X_test_scaled)

# Evaluating the Logistic Regression model
accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)
precision_log_reg = precision_score(y_test, y_pred_log_reg)
recall_log_reg = recall_score(y_test, y_pred_log_reg)
roc_auc_log_reg = roc_auc_score(y_test, y_pred_log_reg)
conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)

# Logistic Regression performance metrics
log_reg_metrics = {
    "Accuracy": accuracy_log_reg,
    "Precision": precision_log_reg,
    "Recall": recall_log_reg,
    "ROC AUC": roc_auc_log_reg
}

log_reg_metrics, conf_matrix_log_reg

"""Random Gforest"""

from sklearn.ensemble import RandomForestClassifier

# Building the Random Forest model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)

# Making predictions on the test set
y_pred_rf = rf.predict(X_test_scaled)

# Evaluating the Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_pred_rf)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)

# Random Forest performance metrics
rf_metrics = {
    "Accuracy": accuracy_rf,
    "Precision": precision_rf,
    "Recall": recall_rf,
    "ROC AUC": roc_auc_rf
}

rf_metrics, conf_matrix_rf

"""Try other models"""

from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Create a Gradient Boosting classifier instance
clf = GradientBoostingClassifier()

# Now, you can use `clf` to fit data and make predictions
# clf.fit(X_train, y_train)
# predictions = clf.predict(X_test)

# Support Vector Machine Model
svm_model = SVC()
svm_model.fit(X_train_scaled, y_train)
y_pred_svm = svm_model.predict(X_test_scaled)

# Gradient Boosting Classifier Model
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train_scaled, y_train)
y_pred_gb = gb_model.predict(X_test_scaled)

# K-Nearest Neighbors Model
knn_model = KNeighborsClassifier()
knn_model.fit(X_train_scaled, y_train)
y_pred_knn = knn_model.predict(X_test_scaled)

# Evaluating the models
models = [svm_model, gb_model, knn_model]
predictions = [y_pred_svm, y_pred_gb, y_pred_knn]
model_names = ['SVM', 'Gradient Boosting', 'K-Nearest Neighbors']
model_performance = {}

for i, model in enumerate(models):
    accuracy = accuracy_score(y_test, predictions[i])
    precision = precision_score(y_test, predictions[i])
    recall = recall_score(y_test, predictions[i])
    roc_auc = roc_auc_score(y_test, predictions[i])
    model_performance[model_names[i]] = {"Accuracy": accuracy, "Precision": precision, "Recall": recall, "ROC AUC": roc_auc}

model_performance

"""Fine tuning gradient and Random forest"""

from sklearn.model_selection import GridSearchCV

# Simplified parameter grid for Gradient Boosting Classifier
param_grid_gb_simplified = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 4]
}

# Creating the simplified Grid Search for Gradient Boosting
gb_grid_search_simplified = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid_gb_simplified, cv=5, scoring='accuracy')
gb_grid_search_simplified.fit(X_train_scaled, y_train)

# Best parameters and best score for Gradient Boosting
best_params_gb_simplified = gb_grid_search_simplified.best_params_
best_score_gb_simplified = gb_grid_search_simplified.best_score_

best_params_gb_simplified, best_score_gb_simplified

"""simplifie drid search for Random forest"""

# Simplified parameter grid for Random Forest
param_grid_rf_simplified = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'min_samples_split': [2, 4]
}

# Creating the simplified Grid Search for Random Forest
rf_grid_search_simplified = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf_simplified, cv=5, scoring='accuracy')
rf_grid_search_simplified.fit(X_train_scaled, y_train)

# Best parameters and best score for Random Forest
best_params_rf_simplified = rf_grid_search_simplified.best_params_
best_score_rf_simplified = rf_grid_search_simplified.best_score_

best_params_rf_simplified, best_score_rf_simplified

"""evaluating tune model on test set"""

# Creating and evaluating the tuned Gradient Boosting Classifier
tuned_gb_model = GradientBoostingClassifier(
    n_estimators=best_params_gb_simplified['n_estimators'],
    learning_rate=best_params_gb_simplified['learning_rate'],
    max_depth=best_params_gb_simplified['max_depth'],
    random_state=42
)
tuned_gb_model.fit(X_train_scaled, y_train)
y_pred_tuned_gb = tuned_gb_model.predict(X_test_scaled)

# Performance metrics for the tuned Gradient Boosting Classifier
accuracy_tuned_gb = accuracy_score(y_test, y_pred_tuned_gb)
precision_tuned_gb = precision_score(y_test, y_pred_tuned_gb)
recall_tuned_gb = recall_score(y_test, y_pred_tuned_gb)
roc_auc_tuned_gb = roc_auc_score(y_test, y_pred_tuned_gb)

tuned_gb_metrics = {
    "Accuracy": accuracy_tuned_gb,
    "Precision": precision_tuned_gb,
    "Recall": recall_tuned_gb,
    "ROC AUC": roc_auc_tuned_gb
}

tuned_gb_metrics

"""evaluating tune model on test set: random forest"""

# Creating and evaluating the tuned Random Forest model
tuned_rf_model = RandomForestClassifier(
    n_estimators=best_params_rf_simplified['n_estimators'],
    max_depth=best_params_rf_simplified['max_depth'],
    min_samples_split=best_params_rf_simplified['min_samples_split'],
    random_state=42
)
tuned_rf_model.fit(X_train_scaled, y_train)
y_pred_tuned_rf = tuned_rf_model.predict(X_test_scaled)

# Performance metrics for the tuned Random Forest
accuracy_tuned_rf = accuracy_score(y_test, y_pred_tuned_rf)
precision_tuned_rf = precision_score(y_test, y_pred_tuned_rf)
recall_tuned_rf = recall_score(y_test, y_pred_tuned_rf)
roc_auc_tuned_rf = roc_auc_score(y_test, y_pred_tuned_rf)

tuned_rf_metrics = {
    "Accuracy": accuracy_tuned_rf,
    "Precision": precision_tuned_rf,
    "Recall": recall_tuned_rf,
    "ROC AUC": roc_auc_tuned_rf
}


"""If the focus is on minimizing false alarms (i.e., high precision), then the Random Forest model might be better."""


# import skl2onnx
# from skl2onnx import convert_sklearn
# from skl2onnx.common.data_types import FloatTensorType
#
# # Specify an initial type for the model ( similar to input shape for the model )
# initial_type = [
#     ( 'input_study_hours' , FloatTensorType( [None,1] ) )
# ]
#
# # Write the ONNX model to disk
# converted_model = convert_sklearn( tuned_rf_model , initial_types=initial_type )
# with open("../../Downloads/sklearn_model.onnx", "wb") as f:
#     f.write( converted_model.SerializeToString() )

# import joblib
#
# # Serializing the tuned Random Forest model
# model_filename = '/tuned_random_forest_model.joblib'
# joblib.dump(tuned_rf_model, model_filename)
#
# model_filename
#
# from google.colab import files
#
# # Assuming 'df' is your cleaned DataFrame
# df.to_csv('cleaned_dataset.csv', index=False)  # Save to Colab environment first
#
# # Download the file to your local machine
# files.download('cleaned_dataset.csv')